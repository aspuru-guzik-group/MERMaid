{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/shixuan_leong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/shixuan_leong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/shixuan_leong/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: will overpenalize shorter sentences\n",
    "\n",
    "def calculate_bleu_score(generated: str, \n",
    "                         ground_truth: str):\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        generated = nltk.word_tokenize(generated)\n",
    "        ground_truth = nltk.word_tokenize(ground_truth)\n",
    "        bleu_score = sentence_bleu([ground_truth], \n",
    "                                   generated, \n",
    "                                   smoothing_function=smoothing_function, \n",
    "                                   weights=(1, 0, 0, 0)) \n",
    "        return round(bleu_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TER (normalized TER) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TER\n",
    "def calculate_ter_score(generated: str, \n",
    "                        ground_truth: str):\n",
    "    edits = edit_distance(ground_truth, generated)\n",
    "    ref_length = len(ground_truth.split())\n",
    "\n",
    "    ter_score = edits / ref_length if ref_length > 0 else float('inf')\n",
    "    return ter_score\n",
    "\n",
    "\n",
    "def edit_distance(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "\n",
    "    d = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,   \n",
    "                           d[i][j - 1] + 1,    \n",
    "                           d[i - 1][j - 1] + cost)  \n",
    "    ter_score = d[len(ref_words)][len(hyp_words)]\n",
    "\n",
    "    return round(ter_score, 3)\n",
    "\n",
    "#normalized TER\n",
    "def calculate_normalized_ter(generated: str, \n",
    "                             ground_truth: str):\n",
    "    \"\"\"Calculate the normalized Translation Edit Rate (TER).\"\"\"\n",
    "    gt_tokens = ground_truth.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    # Calculate the Levenshtein distance\n",
    "    edit_distance = levenshtein_distance(gt_tokens, generated_tokens)\n",
    "\n",
    "    # Calculate normalized TER\n",
    "    if len(gt_tokens) + edit_distance == 0:  # To avoid division by zero\n",
    "        return 0.0\n",
    "    normalized_ter = edit_distance / (len(gt_tokens) + edit_distance)\n",
    "\n",
    "    return round(normalized_ter, 3)\n",
    "\n",
    "def levenshtein_distance(gt_tokens, ocr_tokens):\n",
    "    \"\"\"Calculate the Levenshtein distance between two lists of tokens.\"\"\"\n",
    "    if len(gt_tokens) < len(ocr_tokens):\n",
    "        return levenshtein_distance(ocr_tokens, gt_tokens)\n",
    "\n",
    "    # Create a distance matrix\n",
    "    distances = np.zeros((len(gt_tokens) + 1, len(ocr_tokens) + 1))\n",
    "\n",
    "    # Initialize the distance matrix\n",
    "    for i in range(len(gt_tokens) + 1):\n",
    "        distances[i][0] = i\n",
    "    for j in range(len(ocr_tokens) + 1):\n",
    "        distances[0][j] = j\n",
    "\n",
    "    # Compute the distances\n",
    "    for i in range(1, len(gt_tokens) + 1):\n",
    "        for j in range(1, len(ocr_tokens) + 1):\n",
    "            cost = 0 if gt_tokens[i - 1] == ocr_tokens[j - 1] else 1\n",
    "            distances[i][j] = min(\n",
    "                distances[i - 1][j] + 1,    # Deletion\n",
    "                distances[i][j - 1] + 1,    # Insertion\n",
    "                distances[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    return distances[len(gt_tokens)][len(ocr_tokens)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteor_score(generated: str, \n",
    "                           ground_truth: str):\n",
    "  ground_truth = nltk.word_tokenize(ground_truth)\n",
    "  generated = nltk.word_tokenize(generated)\n",
    "  score = meteor_score([ground_truth], generated)\n",
    "  return round(score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groundtruth_ocr(groundtruth_dict_path, \n",
    "                            ocr_path): \n",
    "    with open(groundtruth_dict_path, 'r') as file:\n",
    "        groundtruth_dict = json.load(file)\n",
    "\n",
    "    with open(ocr_path, 'r') as file:\n",
    "        ocr_caption = json.load(file)\n",
    "\n",
    "    groundtruth_log = {}\n",
    "    ocr_log = []\n",
    "\n",
    "    for key, value in groundtruth_dict.items(): \n",
    "        try: \n",
    "            ocr_value = ocr_caption[key]\n",
    "            bleu_score = calculate_bleu_score(ocr_value, value)\n",
    "            normalized_ter_score = calculate_normalized_ter(ocr_value, value)\n",
    "            meteorscore= calculate_meteor_score(ocr_value, value)\n",
    "            groundtruth_log[key]=bleu_score, normalized_ter_score, meteorscore\n",
    "            ocr_log.append(key)\n",
    "            \n",
    "        except KeyError: \n",
    "            continue \n",
    "\n",
    "    uncompared_gt_captions = [key for key, _ in groundtruth_dict.items() if key not in groundtruth_log]\n",
    "    uncompared_ocr_captions = [ocr_key for ocr_key, _ in ocr_caption.items() if ocr_key not in ocr_log]\n",
    "\n",
    "    if not uncompared_gt_captions and not uncompared_ocr_captions:\n",
    "        return groundtruth_log, None\n",
    "    \n",
    "    uncompared_captions= {\n",
    "    \"uncompared_gt_captions\": uncompared_gt_captions,\n",
    "    \"uncompared_ocr_captions\": uncompared_ocr_captions\n",
    "}\n",
    "\n",
    "    return groundtruth_log, uncompared_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 10.1016_j.gresc.2021.09.003_cleaned.json\n",
      "done processing 10.1016_j.gresc.2021.09.003_cleaned.json\n",
      "done saving 10.1016_j.gresc.2021.09.003_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.gresc.2021.09.003_cleaned.json\n",
      "\n",
      "processing 10.1039_d0sc00031k_cleaned.json\n",
      "done processing 10.1039_d0sc00031k_cleaned.json\n",
      "done saving 10.1039_d0sc00031k_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1039_d4sc02969k_cleaned.json\n",
      "done processing 10.1039_d4sc02969k_cleaned.json\n",
      "done saving 10.1039_d4sc02969k_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1021_acs.joc.8b00486_cleaned.json\n",
      "done processing 10.1021_acs.joc.8b00486_cleaned.json\n",
      "done saving 10.1021_acs.joc.8b00486_cleaned.json\n",
      "done saving unprocessed captions for 10.1021_acs.joc.8b00486_cleaned.json\n",
      "\n",
      "processing 10.1039_c5sc00238a_cleaned.json\n",
      "done processing 10.1039_c5sc00238a_cleaned.json\n",
      "done saving 10.1039_c5sc00238a_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1039_c8sc04482a_cleaned.json\n",
      "done processing 10.1039_c8sc04482a_cleaned.json\n",
      "done saving 10.1039_c8sc04482a_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1016_j.tgchem.2024.100051_cleaned.json\n",
      "done processing 10.1016_j.tgchem.2024.100051_cleaned.json\n",
      "done saving 10.1016_j.tgchem.2024.100051_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.tgchem.2024.100051_cleaned.json\n",
      "\n",
      "processing 10.1021_acs.orglett.2c00362_cleaned.json\n",
      "[Errno 2] No such file or directory: '../ocr_eval_results/captions_ocr/organic_synthesis/10.1021_acs.orglett.2c00362.json_dict.json'\n",
      "processing 10.1039_d4su00258j_cleaned.json\n",
      "done processing 10.1039_d4su00258j_cleaned.json\n",
      "done saving 10.1039_d4su00258j_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1016_j.xcrp.2023.101573_cleaned.json\n",
      "done processing 10.1016_j.xcrp.2023.101573_cleaned.json\n",
      "done saving 10.1016_j.xcrp.2023.101573_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.xcrp.2023.101573_cleaned.json\n",
      "\n",
      "processing 10.1021_acs.orglett.4c02846_cleaned.json\n",
      "done processing 10.1021_acs.orglett.4c02846_cleaned.json\n",
      "done saving 10.1021_acs.orglett.4c02846_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1016_j.isci.2019.07.004_cleaned.json\n",
      "done processing 10.1016_j.isci.2019.07.004_cleaned.json\n",
      "done saving 10.1016_j.isci.2019.07.004_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.isci.2019.07.004_cleaned.json\n",
      "\n",
      "processing 10.1016_j.isci.2022.105896_cleaned.json\n",
      "done processing 10.1016_j.isci.2022.105896_cleaned.json\n",
      "done saving 10.1016_j.isci.2022.105896_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.isci.2022.105896_cleaned.json\n",
      "\n",
      "processing 10.1039_d4sc04222k_cleaned.json\n",
      "done processing 10.1039_d4sc04222k_cleaned.json\n",
      "done saving 10.1039_d4sc04222k_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d4sc04222k_cleaned.json\n",
      "\n",
      "processing 10.1016_j.xcrp.2023.101696_cleaned.json\n",
      "done processing 10.1016_j.xcrp.2023.101696_cleaned.json\n",
      "done saving 10.1016_j.xcrp.2023.101696_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.xcrp.2023.101696_cleaned.json\n",
      "\n",
      "processing 10.1039_c6ob00076b_cleaned.json\n",
      "done processing 10.1039_c6ob00076b_cleaned.json\n",
      "done saving 10.1039_c6ob00076b_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1021_acs.orglett.6b01132_cleaned.json\n",
      "[Errno 2] No such file or directory: '../ocr_eval_results/captions_ocr/organic_synthesis/10.1021_acs.orglett.6b01132.json_dict.json'\n",
      "processing 10.1039_c6ob00108d_cleaned.json\n",
      "done processing 10.1039_c6ob00108d_cleaned.json\n",
      "done saving 10.1039_c6ob00108d_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1039_d3sc00803g_cleaned.json\n",
      "done processing 10.1039_d3sc00803g_cleaned.json\n",
      "done saving 10.1039_d3sc00803g_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d3sc00803g_cleaned.json\n",
      "\n",
      "processing 10.1039_d1ra02225c_cleaned.json\n",
      "done processing 10.1039_d1ra02225c_cleaned.json\n",
      "done saving 10.1039_d1ra02225c_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d1ra02225c_cleaned.json\n",
      "\n",
      "processing 10.1016_j.gresc.2024.09.002_cleaned.json\n",
      "done processing 10.1016_j.gresc.2024.09.002_cleaned.json\n",
      "done saving 10.1016_j.gresc.2024.09.002_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.gresc.2024.09.002_cleaned.json\n",
      "\n",
      "processing 10.1039_d3gc02735j_cleaned.json\n",
      "done processing 10.1039_d3gc02735j_cleaned.json\n",
      "done saving 10.1039_d3gc02735j_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d3gc02735j_cleaned.json\n",
      "\n",
      "processing 10.1002_adsc.202100082_cleaned.json\n",
      "done processing 10.1002_adsc.202100082_cleaned.json\n",
      "done saving 10.1002_adsc.202100082_cleaned.json\n",
      "done saving unprocessed captions for 10.1002_adsc.202100082_cleaned.json\n",
      "\n",
      "processing 10.1039_d4sc00403e_cleaned.json\n",
      "done processing 10.1039_d4sc00403e_cleaned.json\n",
      "done saving 10.1039_d4sc00403e_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d4sc00403e_cleaned.json\n",
      "\n",
      "processing 10.1021_acs.joc.4c00501_cleaned.json\n",
      "done processing 10.1021_acs.joc.4c00501_cleaned.json\n",
      "done saving 10.1021_acs.joc.4c00501_cleaned.json\n",
      "done saving unprocessed captions for 10.1021_acs.joc.4c00501_cleaned.json\n",
      "\n",
      "processing 10.1039_d4ra03939d_cleaned.json\n",
      "done processing 10.1039_d4ra03939d_cleaned.json\n",
      "done saving 10.1039_d4ra03939d_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1039_d1sc04180k_cleaned.json\n",
      "done processing 10.1039_d1sc04180k_cleaned.json\n",
      "done saving 10.1039_d1sc04180k_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1016_j.chempr.2019.12.011_cleaned.json\n",
      "done processing 10.1016_j.chempr.2019.12.011_cleaned.json\n",
      "done saving 10.1016_j.chempr.2019.12.011_cleaned.json\n",
      "done saving unprocessed captions for 10.1016_j.chempr.2019.12.011_cleaned.json\n",
      "\n",
      "processing 10.1002_ejoc.202400146_cleaned.json\n",
      "done processing 10.1002_ejoc.202400146_cleaned.json\n",
      "done saving 10.1002_ejoc.202400146_cleaned.json\n",
      "done saving unprocessed captions for 10.1002_ejoc.202400146_cleaned.json\n",
      "\n",
      "processing 10.1039_d1sc01130h_cleaned.json\n",
      "done processing 10.1039_d1sc01130h_cleaned.json\n",
      "done saving 10.1039_d1sc01130h_cleaned.json\n",
      "done saving unprocessed captions for 10.1039_d1sc01130h_cleaned.json\n",
      "\n",
      "processing 10.1039_c6sc03236b_cleaned.json\n",
      "done processing 10.1039_c6sc03236b_cleaned.json\n",
      "done saving 10.1039_c6sc03236b_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n",
      "processing 10.1039_d4ra04674a_cleaned.json\n",
      "done processing 10.1039_d4ra04674a_cleaned.json\n",
      "done saving 10.1039_d4ra04674a_cleaned.json\n",
      "yay! there's no uncompared captions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field_key = \"organic_synthesis/\"\n",
    "gt_caption_dir = os.path.join(\"../ocr_eval_results/captions_groundtruth/\", field_key)\n",
    "ocr_caption_dir = os.path.join(\"../ocr_eval_results/captions_ocr/\", field_key)\n",
    "bleu_score_dir = os.path.join(\"../ocr_eval_results/ocr_eval/\", field_key)\n",
    "\n",
    "for file in os.listdir(gt_caption_dir): \n",
    "    if file.endswith('_cleaned.json'): \n",
    "        ocr_file = file.replace('_cleaned.json', '.json_dict.json').lower()\n",
    "        gt_file_path = os.path.join(gt_caption_dir, file)\n",
    "        try: \n",
    "            ocr_file_path = os.path.join(ocr_caption_dir, ocr_file)\n",
    "            print(f\"processing {file}\")\n",
    "            groundtruth_log, uncompared_captions = compare_groundtruth_ocr(gt_file_path, ocr_file_path)\n",
    "            print(f\"done processing {file}\")\n",
    "\n",
    "            response_name = file.replace('_cleaned.json', '_bleuscore.json')\n",
    "            output_path = os.path.join(bleu_score_dir, response_name)\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(groundtruth_log, f, indent=4)\n",
    "            print(f\"done saving {file}\")\n",
    "\n",
    "            if uncompared_captions is not None: \n",
    "                uncompared_captions_name = file.replace('_cleaned.json', '_recheck.json')\n",
    "                output_path2 = os.path.join(bleu_score_dir, uncompared_captions_name)\n",
    "                with open(output_path2, 'w') as f:\n",
    "                    json.dump(uncompared_captions, f, indent=4)\n",
    "                print(f\"done saving unprocessed captions for {file}\")\n",
    "                print()\n",
    "            else: \n",
    "                print(f\"yay! there's no uncompared captions\")\n",
    "                print()\n",
    "        except Exception as e: \n",
    "            print(f\"{e}\")\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### additional tests for unprocessed captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"\"\n",
    "ocr_value = \"\"\n",
    "\n",
    "bleu_score = calculate_bleu_score(ocr_value, value)\n",
    "normalized_ter_score = calculate_normalized_ter(ocr_value, value)\n",
    "meteorscore= calculate_meteor_score(ocr_value, value)\n",
    "[bleu_score, normalized_ter_score, meteorscore]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
