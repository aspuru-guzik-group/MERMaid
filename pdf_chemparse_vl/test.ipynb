{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHeck batch API status\n",
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "batch = client.batches.retrieve(\"batch_674bc315e1c4819090f7adf185bed743\")\n",
    "file_response = client.files.content(\"file-3fMqRJD9GSBkk7Jq6pShhi\")\n",
    "print(file_response.text)\n",
    "\n",
    "relevancy_results = {}\n",
    "for line in file_response.text.split(\"\\n\"):\n",
    "    data = json.loads(line)\n",
    "    result = data['response']['body']['choices'][0]['message']['content']\n",
    "    print(result)\n",
    "    custom_id = data['custom_id']\n",
    "    print(custom_id)\n",
    "    relevancy_results[custom_id] = result\n",
    "    print(relevancy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dl_captions_html as dl\n",
    "import os\n",
    "import json \n",
    "\n",
    "input_file_dir = \"../downloaded_html/organic_synthesis/\"\n",
    "caption_file_dir = \"../downloaded_captions/organic_synthesis/\"\n",
    "article_name = \"10.1039_d3gc02735j.json\"\n",
    "input_file_path = os.path.join(input_file_dir, article_name)\n",
    "base_name = \"10.1039_d3gc02735j_with_footnotes\"\n",
    "\n",
    "all_captions= dl.download_captions_headers_footnotes_rsc(input_file_path)\n",
    "dl.save_captions(caption_file_dir, article_name, all_captions)\n",
    "\n",
    "# for file in os.listdir(input_file_dir):\n",
    "#     if file.endswith('.json'): \n",
    "#         article_name = file.removesuffix('.json') + '_with_footnotes'\n",
    "#         input_file_path = os.path.join(input_file_dir, file)\n",
    "#         if \"10.1021\" in file:\n",
    "#             all_captions = dl.download_captions_headers_footnotes_acs(input_file_path)\n",
    "#             dl.save_captions(caption_file_dir, article_name, all_captions)\n",
    "        \n",
    "#         if \"10.1002\" in file:\n",
    "#             all_captions = dl.download_captions_headers_footnotes_wiley(input_file_path) \n",
    "#             dl.save_captions(caption_file_dir, article_name, all_captions)\n",
    "\n",
    "#         if \"10.1039\" in file:\n",
    "#             all_captions = dl.download_captions_headers_footnotes_rsc(input_file_path)\n",
    "#             dl.save_captions(caption_file_dir, article_name, all_captions)\n",
    "\n",
    "#         if \"10.1016\" in file: \n",
    "#             all_captions = dl.download_captions_headers_footnots_elsevier_cellpress(input_file_path)\n",
    "#             dl.save_captions(caption_file_dir, article_name, all_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_file_dir = \"/mnt/c/Users/Shi Xuan/OneDrive - University of Toronto/ocr-captions_generated/photocatalysis/10.1002_anie.201805732.json\"\n",
    "for filename in os.listdir(input_file_dir):\n",
    "    if \"10.1002.\" in filename:\n",
    "        new_filename = filename.replace(\"10.1002.\", \"10.1002_\", 1)\n",
    "    elif \"10.1039.\" in filename: \n",
    "        new_filename = filename.replace(\"10.1039.\", \"10.1039_\", 1)\n",
    "    elif \"10.1021.\" in filename: \n",
    "        new_filename = filename.replace(\"10.1021.\", \"10.1021_\", 1)    \n",
    "    elif \"10.1016.\" in filename: \n",
    "        new_filename = filename.replace(\"10.1016.\", \"10.1016_\", 1)        \n",
    "    old_filepath = os.path.join(input_file_dir, filename)\n",
    "    new_filepath = os.path.join(input_file_dir, new_filename)\n",
    "    os.rename(old_filepath, new_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "original_directory = \"/mnt/c/Users/Shi Xuan/OneDrive - University of Toronto/0_Progress slides/\"\n",
    "new_directory = \"/mnt/c/Users/Shi Xuan/OneDrive - University of Toronto/0_Progress slides/temp/\"\n",
    "\n",
    "\n",
    "for file in os.listdir(original_directory):\n",
    "    if file.endswith('json'):\n",
    "        print(f\"processing {file}\")\n",
    "        file_path = os.path.join(original_directory, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            cleaned_data = sorted([item[\"<OCR>\"].replace('\"', '') for item in data])\n",
    "        \n",
    "        new_file_path = os.path.join(new_directory, file)\n",
    "        with open(new_file_path, \"w\") as new_file:\n",
    "            json.dump(cleaned_data, new_file, indent=4)\n",
    "        print(f\"done processing {file}\")\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = \"\"\"\n",
    "\n",
    "[<div bis_skin_checked=\"1\" class=\"tables frame-topbot colsep-0 rowsep-0\" id=\"tbl1\"><span class=\"captions text-s\"><span id=\"cap0030\"><p id=\"tspara0010\"><span class=\"label\">Table 1</span>. Optimization of the reaction conditions</p></span></span><div bis_skin_checked=\"1\" class=\"groups\"><table><thead><tr class=\"rowsep-1 valign-bottom\"><th colspan=\"7\" scope=\"col\"><figure class=\"inline-figure\"><img alt=\"\" height=\"76\" src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2589004222021691-fx2.jpg\"/></figure></th></tr><tr class=\"rowsep-1 valign-bottom\"><th scope=\"col\">Entry</th><th scope=\"col\">Rh catalyst<a class=\"anchor anchor-primary\" data-sd-ui-side-panel-opener=\"true\" data-xocs-content-id=\"tblfn1\" data-xocs-content-type=\"reference\" href=\"#tblfn1\" name=\"btblfn1\"><span class=\"anchor-text-container\"><span class=\"anchor-text\"><sup>a</sup></span></span></a></th><th scope=\"col\">Base</th><th scope=\"col\">Solvent</th><th scope=\"col\">Temp.</th><th scope=\"col\">Yield</th><th scope=\"col\"><em>ee</em></th></tr></thead><tbody><tr class=\"valign-top\"><td>1</td><td>Rh<sub>2</sub>(<em>S</em>-DOSP)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>58%</td><td>59%</td></tr><tr class=\"valign-top\"><td>2</td><td>Rh<sub>2</sub>(<em>S</em>-PTTL)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>66%</td><td>87%</td></tr><tr class=\"valign-top\"><td>3</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>61%</td><td>91%</td></tr><tr class=\"valign-top\"><td>4</td><td>Rh<sub>2</sub>(<em>S</em>-BTPCP)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td><em>N.D.</em></td><td>–</td></tr><tr class=\"valign-top\"><td>5</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td>K<sub>2</sub>CO<sub>3</sub></td><td>TFT</td><td>25°C</td><td><em>N</em>.<em>R</em>.</td><td>–</td></tr><tr class=\"valign-top\"><td>6</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td>CsCO<sub>3</sub></td><td>TFT</td><td>25°C</td><td><em>N</em>.<em>R</em>.</td><td>–</td></tr><tr class=\"valign-top\"><td>7</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td><em>t</em>BuOK</td><td>TFT</td><td>25°C</td><td><em>N</em>.<em>R</em>.</td><td>–</td></tr><tr class=\"valign-top\"><td>8</td><td>Rh<sub>2</sub>(S-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>Toluene</td><td>25°C</td><td>53%</td><td>82%</td></tr><tr class=\"valign-top\"><td>9</td><td>Rh<sub>2</sub>(S-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>DCM</td><td>25°C</td><td>47%</td><td>20%</td></tr><tr class=\"valign-top\"><td>10</td><td>Rh<sub>2</sub>(S-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>DMF</td><td>25°C</td><td>Trace</td><td>–</td></tr><tr class=\"valign-top\"><td>11</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>40°C</td><td>73%</td><td>87%</td></tr><tr class=\"valign-top\"><td>12</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (1 mol %)</td><td>DIPEA</td><td>TFT</td><td>0°C</td><td>21%</td><td>–</td></tr><tr class=\"valign-top\"><td>13</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (0.5 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>61%</td><td>92%</td></tr><tr class=\"valign-top\"><td>14</td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (0.25 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>50%</td><td>74%</td></tr><tr class=\"valign-top\"><td>15<a class=\"anchor anchor-primary\" data-sd-ui-side-panel-opener=\"true\" data-xocs-content-id=\"tblfn2\" data-xocs-content-type=\"reference\" href=\"#tblfn2\" name=\"btblfn2\"><span class=\"anchor-text-container\"><span class=\"anchor-text\"><sup>b</sup></span></span></a></td><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub> (0.5 mol %)</td><td>DIPEA</td><td>TFT</td><td>25°C</td><td>93%</td><td>95%</td></tr><tr class=\"valign-top\"><td colspan=\"7\"><figure class=\"inline-figure\"><img alt=\"\" height=\"156\" src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2589004222021691-fx3.jpg\"/></figure></td></tr></tbody></table></div><div bis_skin_checked=\"1\" class=\"legend\"><div bis_skin_checked=\"1\" class=\"u-margin-s-bottom\" id=\"tspara0015\"><em>N.D.</em>, Not detect.</div></div><dl class=\"footnotes\"><dt id=\"tblfn1\">a</dt><dd><div bis_skin_checked=\"1\" class=\"u-margin-s-bottom\" id=\"ntpara0020\">Reaction conditions: <em>N</em>-Triftosylhydrazone <strong>1</strong> (0.2 mmol), 1,1-diphenylethylene <strong>2</strong> (0.1 mmol), [Rh] catalyst (1 mol %), DIPEA (<em>N</em>, <em>N</em>-diisopropylethylamine) (0.2 mmol), TFT (benzotrifluoride, 3.0 mL), 12 h, under N<sub>2</sub>.</div></dd><dt id=\"tblfn2\">b</dt><dd><div bis_skin_checked=\"1\" class=\"u-margin-s-bottom\" id=\"ntpara0025\"><em>α</em>-Methyl-4-NO<sub>2</sub>-phenylethylene was used.</div></dd></dl></div>, <div bis_skin_checked=\"1\" class=\"tables frame-topbot colsep-0 rowsep-0\" id=\"undtbl1\"><div bis_skin_checked=\"1\" class=\"groups\"><table><thead><tr class=\"rowsep-1 valign-bottom\"><th scope=\"col\">REAGENT or RESOURCE</th><th scope=\"col\">SOURCE</th><th scope=\"col\">IDENTIFIER</th></tr></thead><tbody><tr class=\"rowsep-1 valign-top\"><th colspan=\"3\" scope=\"row\"><strong>Chemicals, peptides, and recombinant proteins</strong></th></tr><tr class=\"valign-top\"><td>2-(Trifluoromethyl)benzenesulfonyl chloride</td><td>776-04-5</td><td>Energy Chemical</td></tr><tr class=\"valign-top\"><td>DIPEA</td><td>7087-68-5</td><td>Energy Chemical</td></tr><tr class=\"valign-top\"><td>Rh<sub>2</sub>(<em>S</em>-PTAD)<sub>4</sub></td><td>909389-99-7</td><td>J&amp;K Scientific</td></tr><tr class=\"valign-top\"><td>TFT(Benzotrifluoride)</td><td>98-08-8</td><td>Energy Chemical</td></tr><tr class=\"valign-top\"><td>Hydrazinium hydroxide solution</td><td>10217-52-4</td><td>Energy Chemical</td></tr></tbody></table></div></div>]\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "caption_text = soup.find(\"p\").get_text(strip=True)\n",
    "print(caption_text)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content2 = \"\"\"\n",
    "<figure class=\"figure text-xs\" id=\"sch1\"><span><img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S2666386423005271-sc1.jpg\" height=\"381\" alt=\"\" aria-describedby=\"cap0010\"><ol class=\"u-margin-s-bottom\"><li><a class=\"anchor download-link u-font-sans anchor-primary\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S2666386423005271-sc1_lrg.jpg\" target=\"_blank\" download=\"\" title=\"Download high-res image (945KB)\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Download: <span class=\"download-link-title\">Download high-res image (945KB)</span></span></span></a></li><li><a class=\"anchor download-link u-font-sans anchor-primary\" href=\"https://ars.els-cdn.com/content/image/1-s2.0-S2666386423005271-sc1.jpg\" target=\"_blank\" download=\"\" title=\"Download full-size image\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Download: <span class=\"download-link-title\">Download full-size image</span></span></span></a></li></ol></span><span class=\"captions text-s\"><span id=\"cap0010\"><p id=\"fspara0010\"><span class=\"label\">Scheme 1</span>. The importance of <a href=\"/topics/pharmacology-toxicology-and-pharmaceutical-science/deuterium\" title=\"Learn more about deuterium from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">deuterium</a><span>, applications of deuteration in <a href=\"/topics/pharmacology-toxicology-and-pharmaceutical-science/drug-discovery\" title=\"Learn more about drug discovery from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">drug discovery</a> and strategies to access D-labeled alkyl methyl sulfides</span></p><div class=\"u-margin-s-bottom\" id=\"fspara0015\" bis_skin_checked=\"1\"><span>(A) The importance of <a href=\"/topics/chemistry/deuterium\" title=\"Learn more about deuterium from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">deuterium</a> in </span><a href=\"/topics/pharmacology-toxicology-and-pharmaceutical-science/drug-development\" title=\"Learn more about drug development from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">drug development</a> and discovery: combination with magic methyl effect and heteroatom.</div><div class=\"u-margin-s-bottom\" id=\"fspara0020\" bis_skin_checked=\"1\"><span>(B) <a href=\"/topics/chemistry/hydrogen-isotope\" title=\"Learn more about Hydrogen isotope from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">Hydrogen isotope</a> exchange (HIE) by gaseous D</span><sub>2</sub>.</div><div class=\"u-margin-s-bottom\" id=\"fspara0025\" bis_skin_checked=\"1\">(C) Nucleophilic deuterated <a href=\"/topics/chemistry/methylation\" title=\"Learn more about methylation from ScienceDirect's AI-generated Topic Pages\" class=\"topic-link\">methylation</a> from thiol.</div><div class=\"u-margin-s-bottom\" id=\"fspara0030\" bis_skin_checked=\"1\">(D) ArSO<sub>2</sub>SNa: \"S\" shuttle to access D-labeled methylthiolated molecules (this work).</div></span></span></figure>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content2, 'html.parser')\n",
    "\n",
    "figure_captions = soup.find_all(\"figure\", class_=\"figure text-xs\")\n",
    "\n",
    "for caption in figure_captions:\n",
    "    caption_text = caption.find(\"p\").get_text(strip=True)\n",
    "    print(caption_text)\n",
    "    print()\n",
    "    footnotes = caption.find_all(\"div\", class_=\"u-margin-s-bottom\")\n",
    "    for footnote in footnotes:\n",
    "        footnote_a = footnote.get_text(strip=True)\n",
    "        print(footnote_a)\n",
    "        caption_text += f\" {footnote_a}\" \n",
    "    print(caption_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def process_json_lines(input_file, output_dir):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Open the input JSON file and read line by line\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Load the JSON object from the line\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Replace / with _ in the image name to use as filename\n",
    "                filename = line.replace('/', '_')\n",
    "                \n",
    "                # Create the path for the empty JSON file\n",
    "                output_file = os.path.join(output_dir, filename + '.json')\n",
    "                \n",
    "                # Save an empty JSON object to the new file\n",
    "                with open(output_file, 'w') as outfile:\n",
    "                    json.dump({}, outfile)\n",
    "                \n",
    "                print(f\"Created empty JSON file: {output_file}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse line: {line}\")\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = '../downloaded_html/electrosynthesis/DOI.json'  \n",
    "output_dir = '../downloaded_html/electrosynthesis/'\n",
    "process_json_lines(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../captions_ocr/electrosynthesis\"\n",
    "output_dir = input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        # Open and load the JSON file\n",
    "        with open(os.path.join(input_dir, filename), 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Replace \"Caption\" with an empty space in each entry\n",
    "        modified_data = [entry.replace(\"Caption\", \" \") for entry in data]\n",
    "        \n",
    "        # Save the modified data to the output directory with the same filename\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        with open(output_path, 'w') as output_file:\n",
    "            json.dump(modified_data, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "\n",
    "def convert_list_to_dict(input_dir, output_dir): \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define a list of possible keywords with acceptable spellings\n",
    "    keywords = [\"Scheme\", \"Figure\", \"Table\", \"Chart\", \"Fig\"]\n",
    "\n",
    "    # Function to match an entry with keywords using fuzzy matching\n",
    "    def find_keyword(entry):\n",
    "        for keyword in keywords:\n",
    "            if fuzz.partial_ratio(entry, keyword) >= 80:\n",
    "                return keyword\n",
    "        return None\n",
    "\n",
    "    pattern = r\"((Scheme|Figure|Table|Shemme|Chart|Fig)\\s*[A-Za-z0-9]+)\"\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        base_filename = filename.removesuffix(\"_with_footnotes_captions.json\") + \"_dict.json\"\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(input_dir, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "            result_dict = {}\n",
    "    \n",
    "            for entry in data:\n",
    "                match = re.search(pattern, entry, re.IGNORECASE)\n",
    "                if match:\n",
    "                    key = match.group(0).strip()\n",
    "                else:\n",
    "                    keyword_match = find_keyword(entry)\n",
    "                    if keyword_match:\n",
    "                        keyword_pos = entry.lower().find(keyword_match.lower())\n",
    "                        key = entry[keyword_pos:].split()[0]\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                value = entry[len(key):].strip(\" .\")\n",
    "                result_dict[key] = value\n",
    "\n",
    "            output_path = os.path.join(output_dir, base_filename)\n",
    "            with open(output_path, 'w') as output_file:\n",
    "                json.dump(result_dict, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_list_to_dict(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare ground truth and OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: will overpenalize shorter sentences\n",
    "\n",
    "def calculate_bleu_score(generated: str, \n",
    "                         ground_truth: str):\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        generated = nltk.word_tokenize(generated)\n",
    "        ground_truth = nltk.word_tokenize(ground_truth)\n",
    "        bleu_score = sentence_bleu([ground_truth], \n",
    "                                   generated, \n",
    "                                   smoothing_function=smoothing_function, \n",
    "                                   weights=(1, 0, 0, 0)) \n",
    "        return round(bleu_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TER score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ter_score(generated: str, \n",
    "                        ground_truth: str):\n",
    "    edits = edit_distance(ground_truth, generated)\n",
    "    ref_length = len(ground_truth.split())\n",
    "\n",
    "    ter_score = edits / ref_length if ref_length > 0 else float('inf')\n",
    "    return ter_score\n",
    "\n",
    "\n",
    "def edit_distance(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "\n",
    "    d = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,   \n",
    "                           d[i][j - 1] + 1,    \n",
    "                           d[i - 1][j - 1] + cost)  \n",
    "    ter_score = d[len(ref_words)][len(hyp_words)]\n",
    "\n",
    "    return round(ter_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### normalized TER score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(gt_tokens, ocr_tokens):\n",
    "    \"\"\"Calculate the Levenshtein distance between two lists of tokens.\"\"\"\n",
    "    if len(gt_tokens) < len(ocr_tokens):\n",
    "        return levenshtein_distance(ocr_tokens, gt_tokens)\n",
    "\n",
    "    # Create a distance matrix\n",
    "    distances = np.zeros((len(gt_tokens) + 1, len(ocr_tokens) + 1))\n",
    "\n",
    "    # Initialize the distance matrix\n",
    "    for i in range(len(gt_tokens) + 1):\n",
    "        distances[i][0] = i\n",
    "    for j in range(len(ocr_tokens) + 1):\n",
    "        distances[0][j] = j\n",
    "\n",
    "    # Compute the distances\n",
    "    for i in range(1, len(gt_tokens) + 1):\n",
    "        for j in range(1, len(ocr_tokens) + 1):\n",
    "            cost = 0 if gt_tokens[i - 1] == ocr_tokens[j - 1] else 1\n",
    "            distances[i][j] = min(\n",
    "                distances[i - 1][j] + 1,    # Deletion\n",
    "                distances[i][j - 1] + 1,    # Insertion\n",
    "                distances[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    return distances[len(gt_tokens)][len(ocr_tokens)]\n",
    "\n",
    "def calculate_normalized_ter(generated: str, \n",
    "                             ground_truth: str):\n",
    "    \"\"\"Calculate the normalized Translation Edit Rate (TER).\"\"\"\n",
    "    gt_tokens = ground_truth.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    # Calculate the Levenshtein distance\n",
    "    edit_distance = levenshtein_distance(gt_tokens, generated_tokens)\n",
    "\n",
    "    # Calculate normalized TER\n",
    "    if len(gt_tokens) + edit_distance == 0:  # To avoid division by zero\n",
    "        return 0.0\n",
    "    normalized_ter = edit_distance / (len(gt_tokens) + edit_distance)\n",
    "\n",
    "    return round(normalized_ter, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteor_score(generated: str, \n",
    "                           ground_truth: str):\n",
    "  ground_truth = nltk.word_tokenize(ground_truth)\n",
    "  generated = nltk.word_tokenize(generated)\n",
    "  score = meteor_score([ground_truth], generated)\n",
    "  return round(score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = \"I really do love orange\"\n",
    "ground_truth = \"I really do love apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = calculate_meteor_score(generated, ground_truth)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groundtruth_ocr(groundtruth_dict_path, \n",
    "                            ocr_path): \n",
    "    with open(groundtruth_dict_path, 'r') as file:\n",
    "        groundtruth_dict = json.load(file)\n",
    "\n",
    "    with open(ocr_path, 'r') as file:\n",
    "        ocr_caption = json.load(file)\n",
    "\n",
    "    groundtruth_log = {}\n",
    "    ocr_log = []\n",
    "\n",
    "    for key, value in groundtruth_dict.items(): \n",
    "        try: \n",
    "            ocr_value = ocr_caption[key]\n",
    "            bleu_score = calculate_bleu_score(ocr_value, value)\n",
    "            normalized_ter_score = calculate_normalized_ter(ocr_value, value)\n",
    "            meteorscore= calculate_meteor_score(ocr_value, value)\n",
    "            groundtruth_log[key]=bleu_score, normalized_ter_score, meteorscore\n",
    "            ocr_log.append(key)\n",
    "            \n",
    "        except KeyError: \n",
    "            continue \n",
    "\n",
    "    uncompared_gt_captions = [key for key, _ in groundtruth_dict.items() if key not in groundtruth_log]\n",
    "    uncompared_ocr_captions = [ocr_key for ocr_key, _ in ocr_caption.items() if ocr_key not in ocr_log]\n",
    "\n",
    "    uncompared_captions= {\n",
    "    \"uncompared_gt_captions\": uncompared_gt_captions,\n",
    "    \"uncompared_ocr_captions\": uncompared_ocr_captions\n",
    "}\n",
    "\n",
    "    return groundtruth_log, uncompared_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "gt_caption_dir = \"./\"\n",
    "ocr_caption_dir = \"../\"\n",
    "bleu_score_dir = \"../\"\n",
    "\n",
    "for file in os.listdir(gt_caption_dir): \n",
    "    if file.endswith('.json'): \n",
    "        #ocr_name = file.replace('.json', '')\n",
    "        gt_file_path = os.path.join(gt_caption_dir, file)\n",
    "        ocr_file_path = os.path.join(ocr_caption_dir, file)\n",
    "        print(f\"processing {file}\")\n",
    "        groundtruth_log, uncompared_captions = compare_groundtruth_ocr(gt_file_path, ocr_file_path)\n",
    "        print(groundtruth_log)\n",
    "        print(uncompared_captions)\n",
    "        print(f\"done processing {file}\")\n",
    "\n",
    "        response_name = file.replace('.json', '_bleuscore.json')\n",
    "        output_path = os.path.join(bleu_score_dir, response_name)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(groundtruth_log, f, indent=4)\n",
    "        print(f\"done saving {file}\")\n",
    "\n",
    "        uncompared_captions_name = file.replace('.json', '_recheck.json')\n",
    "        output_path2 = os.path.join(bleu_score_dir, uncompared_captions_name)\n",
    "        with open(output_path2, 'w') as f:\n",
    "            json.dump(uncompared_captions, f, indent=4)\n",
    "        print(f\"done saving unprocessed captions for {file}\")\n",
    "        print()\n",
    "             \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:PDF-TF-chem/test.ipynb
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> main:pdf_chemparse_vl/test.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import html\n",
    "\n",
    "def clean_text(text):\n",
    "    # Decode HTML entities\n",
    "    #decoded_text = html.unescape(text)\n",
    "    # Remove special characters using regex, allowing only alphanumeric and basic punctuation\n",
    "    \n",
    "    cleaned_text=text\n",
    "    # Remove specific unwanted encoded characters\n",
    "    cleaned_text = cleaned_text.replace('\\u2009', ' ')  # Thin space\n",
    "    cleaned_text = cleaned_text.replace('\\u2005', ' ')  # Four-per-em space\n",
    "    cleaned_text = cleaned_text.replace('\\u2212', '-')  # Minus sign\n",
    "    cleaned_text = cleaned_text.replace('\\u2013', '-')  # En dash\n",
    "    cleaned_text = cleaned_text.replace('\\u2014', '-')  # Em dash\n",
    "    cleaned_text = cleaned_text.replace('\\u2018', '\\'')  # Left single quotation mark\n",
    "    cleaned_text = cleaned_text.replace('\\u2019', '\\'')  # Right single quotation mark\n",
    "    cleaned_text = cleaned_text.replace('\\u2026', '...')  # Ellipsis\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s.,;()\\-]', '', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_json_file(input_file_path, output_file_path):\n",
    "    # Load the JSON data\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Clean each entry in the dictionary\n",
    "    cleaned_data = {key: clean_text(value) for key, value in data.items()}\n",
    "\n",
    "    # Save the cleaned data back to a new JSON file\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        json.dump(cleaned_data, file, indent=4)\n",
    "\n",
    "# # Example usage\n",
    "# input_json_file = '../ocr_eval_results/captions_groundtruth/photocatalysis/10.1002_anie.202110257.json'  # Replace with your input file path\n",
    "# output_json_file = '../ocr_eval_results/captions_groundtruth/photocatalysis/10.1002_anie.202110257_test.json'\n",
    "# clean_json_file(input_json_file, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:PDF-TF-chem/test.ipynb
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> main:pdf_chemparse_vl/test.ipynb
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "input_dir = \"../ocr_eval_results/captions_groundtruth/electrosynthesis/\"\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith('.json'): \n",
    "        output_file = file.replace('.json', '_cleaned.json')\n",
    "        input_json_file = os.path.join(input_dir, file)\n",
    "        output_json_file = os.path.join(input_dir, output_file)\n",
    "        clean_json_file(input_json_file, output_json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heinsight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
