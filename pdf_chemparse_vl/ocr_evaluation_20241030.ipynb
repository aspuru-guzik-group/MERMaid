{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: will overpenalize shorter sentences\n",
    "\n",
    "def calculate_bleu_score(generated: str, \n",
    "                         ground_truth: str):\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        generated = nltk.word_tokenize(generated)\n",
    "        ground_truth = nltk.word_tokenize(ground_truth)\n",
    "        bleu_score = sentence_bleu([ground_truth], \n",
    "                                   generated, \n",
    "                                   smoothing_function=smoothing_function, \n",
    "                                   weights=(1, 0, 0, 0)) \n",
    "        return round(bleu_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TER (normalized TER) scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TER\n",
    "def calculate_ter_score(generated: str, \n",
    "                        ground_truth: str):\n",
    "    edits = edit_distance(ground_truth, generated)\n",
    "    ref_length = len(ground_truth.split())\n",
    "\n",
    "    ter_score = edits / ref_length if ref_length > 0 else float('inf')\n",
    "    return ter_score\n",
    "\n",
    "\n",
    "def edit_distance(ref, hyp):\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "\n",
    "    d = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,   \n",
    "                           d[i][j - 1] + 1,    \n",
    "                           d[i - 1][j - 1] + cost)  \n",
    "    ter_score = d[len(ref_words)][len(hyp_words)]\n",
    "\n",
    "    return round(ter_score, 3)\n",
    "\n",
    "#normalized TER\n",
    "def calculate_normalized_ter(generated: str, \n",
    "                             ground_truth: str):\n",
    "    \"\"\"Calculate the normalized Translation Edit Rate (TER).\"\"\"\n",
    "    gt_tokens = ground_truth.split()\n",
    "    generated_tokens = generated.split()\n",
    "\n",
    "    # Calculate the Levenshtein distance\n",
    "    edit_distance = levenshtein_distance(gt_tokens, generated_tokens)\n",
    "\n",
    "    # Calculate normalized TER\n",
    "    if len(gt_tokens) + edit_distance == 0:  # To avoid division by zero\n",
    "        return 0.0\n",
    "    normalized_ter = edit_distance / (len(gt_tokens) + edit_distance)\n",
    "\n",
    "    return round(normalized_ter, 3)\n",
    "\n",
    "def levenshtein_distance(gt_tokens, ocr_tokens):\n",
    "    \"\"\"Calculate the Levenshtein distance between two lists of tokens.\"\"\"\n",
    "    if len(gt_tokens) < len(ocr_tokens):\n",
    "        return levenshtein_distance(ocr_tokens, gt_tokens)\n",
    "\n",
    "    # Create a distance matrix\n",
    "    distances = np.zeros((len(gt_tokens) + 1, len(ocr_tokens) + 1))\n",
    "\n",
    "    # Initialize the distance matrix\n",
    "    for i in range(len(gt_tokens) + 1):\n",
    "        distances[i][0] = i\n",
    "    for j in range(len(ocr_tokens) + 1):\n",
    "        distances[0][j] = j\n",
    "\n",
    "    # Compute the distances\n",
    "    for i in range(1, len(gt_tokens) + 1):\n",
    "        for j in range(1, len(ocr_tokens) + 1):\n",
    "            cost = 0 if gt_tokens[i - 1] == ocr_tokens[j - 1] else 1\n",
    "            distances[i][j] = min(\n",
    "                distances[i - 1][j] + 1,    # Deletion\n",
    "                distances[i][j - 1] + 1,    # Insertion\n",
    "                distances[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "\n",
    "    return distances[len(gt_tokens)][len(ocr_tokens)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_meteor_score(generated: str, \n",
    "                           ground_truth: str):\n",
    "  ground_truth = nltk.word_tokenize(ground_truth)\n",
    "  generated = nltk.word_tokenize(generated)\n",
    "  score = meteor_score([ground_truth], generated)\n",
    "  return round(score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groundtruth_ocr(groundtruth_dict_path, \n",
    "                            ocr_path): \n",
    "    with open(groundtruth_dict_path, 'r') as file:\n",
    "        groundtruth_dict = json.load(file)\n",
    "\n",
    "    with open(ocr_path, 'r') as file:\n",
    "        ocr_caption = json.load(file)\n",
    "\n",
    "    groundtruth_log = {}\n",
    "    ocr_log = []\n",
    "\n",
    "    for key, value in groundtruth_dict.items(): \n",
    "        try: \n",
    "            ocr_value = ocr_caption[key]\n",
    "            bleu_score = calculate_bleu_score(ocr_value, value)\n",
    "            normalized_ter_score = calculate_normalized_ter(ocr_value, value)\n",
    "            meteorscore= calculate_meteor_score(ocr_value, value)\n",
    "            groundtruth_log[key]=bleu_score, normalized_ter_score, meteorscore\n",
    "            ocr_log.append(key)\n",
    "            \n",
    "        except KeyError: \n",
    "            continue \n",
    "\n",
    "    uncompared_gt_captions = [key for key, _ in groundtruth_dict.items() if key not in groundtruth_log]\n",
    "    uncompared_ocr_captions = [ocr_key for ocr_key, _ in ocr_caption.items() if ocr_key not in ocr_log]\n",
    "\n",
    "    if not uncompared_gt_captions and not uncompared_ocr_captions:\n",
    "        return groundtruth_log, None\n",
    "    \n",
    "    uncompared_captions= {\n",
    "    \"uncompared_gt_captions\": uncompared_gt_captions,\n",
    "    \"uncompared_ocr_captions\": uncompared_ocr_captions\n",
    "}\n",
    "\n",
    "    return groundtruth_log, uncompared_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_key = \"organic_synthesis/\"\n",
    "gt_caption_dir = os.path.join(\"../ocr_eval_results/captions_groundtruth/\", field_key)\n",
    "ocr_caption_dir = os.path.join(\"../ocr_eval_results/captions_ocr/\", field_key)\n",
    "bleu_score_dir = os.path.join(\"../ocr_eval_results/ocr_eval/\", field_key)\n",
    "\n",
    "for file in os.listdir(gt_caption_dir): \n",
    "    if file.endswith('_cleaned.json'): \n",
    "        ocr_file = file.replace('_cleaned.json', '.json_dict.json').lower()\n",
    "        gt_file_path = os.path.join(gt_caption_dir, file)\n",
    "        try: \n",
    "            ocr_file_path = os.path.join(ocr_caption_dir, ocr_file)\n",
    "            print(f\"processing {file}\")\n",
    "            groundtruth_log, uncompared_captions = compare_groundtruth_ocr(gt_file_path, ocr_file_path)\n",
    "            print(f\"done processing {file}\")\n",
    "\n",
    "            response_name = file.replace('_cleaned.json', '_bleuscore.json')\n",
    "            output_path = os.path.join(bleu_score_dir, response_name)\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(groundtruth_log, f, indent=4)\n",
    "            print(f\"done saving {file}\")\n",
    "\n",
    "            if uncompared_captions is not None: \n",
    "                uncompared_captions_name = file.replace('_cleaned.json', '_recheck.json')\n",
    "                output_path2 = os.path.join(bleu_score_dir, uncompared_captions_name)\n",
    "                with open(output_path2, 'w') as f:\n",
    "                    json.dump(uncompared_captions, f, indent=4)\n",
    "                print(f\"done saving unprocessed captions for {file}\")\n",
    "                print()\n",
    "            else: \n",
    "                print(f\"yay! there's no uncompared captions\")\n",
    "                print()\n",
    "        except Exception as e: \n",
    "            print(f\"{e}\")\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### additional tests for unprocessed captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"Scheme 1 Synthesis of phosphorothiolated heterocycles\"\n",
    "ocr_value = \"Scheme 1 Synthesis of phosphorothiolated heterocycles\"\n",
    "\n",
    "bleu_score = calculate_bleu_score(ocr_value, value)\n",
    "normalized_ter_score = calculate_normalized_ter(ocr_value, value)\n",
    "meteorscore= calculate_meteor_score(ocr_value, value)\n",
    "[bleu_score, normalized_ter_score, meteorscore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENCHEMIE TEST\n",
    "\n",
    "str1 = \" Scope of the reactiona. \"\n",
    "str2 = \"Scope of the reactiona.aReaction condition: 4-bromophenyl tosylate (1)(1 mmol); ArB(OH)2/Ar-olefins (1.2 mmol); azoles (1.2 mmol); Pd(OAc)2 (7mol%); CuI (1.2 mmol); K2CO3 (1.2 mmol) for only Suzuki cross-coupling; [ETMG][EtS]/H2O (9:1) = 10 mL.\"\n",
    "print(len(str1))\n",
    "print(len(str2))\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1, set2 = set(str1), set(str2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union if union > 0 else 0.0\n",
    "    return (round(similarity, 2))\n",
    "jaccard_similarity(str1, str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"total\": 6,\n",
    "        \"TP\": 1,\n",
    "        \"FN\": 5,\n",
    "        \"FP\": 0\n",
    "    },\n",
    "    {\n",
    "        \"Scheme 1\": \"Approaches for the Indirect (i-HAT) or Direct (d-HAT) Photocatalyzed Hydrogen Atom Transfer Reaction\",\n",
    "        \"Figure 1\": \"Mechanistic investigation: (a) deuteration experiment for KIE determination; (b) redox potentials of the involved species; (c) mechanistic proposal.\"\n",
    "    },\n",
    "    {}\n",
    "]\n",
    "results = data[0]\n",
    "filename= \"test\"\n",
    "results[\"filename\"] = filename\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "save_path = \"../openchemie/all_results.csv\"\n",
    "CATEGORIES = [\"organic_synthesis\", \"electrosynthesis\", \"photocatalysis\"]\n",
    "all_results = []\n",
    "for category in CATEGORIES:\n",
    "    dir = \"../openchemie/\"+ category + \"/\" + \"eval\"\n",
    "    for file in os.listdir(dir): \n",
    "        name = file.removesuffix(\"_caption_evaluation.json\")\n",
    "        filepath = dir + \"/\" + file\n",
    "        with open(filepath, 'r') as f: \n",
    "            data = json.load(f)\n",
    "            results = data[0]\n",
    "            results[\"filename\"] = name\n",
    "            all_results.append(results)\n",
    "df = pd.DataFrame(all_results)\n",
    "with open(save_path, 'w') as f: \n",
    "    df.to_csv(f, index=False)\n",
    "print(\"done saving\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mermes_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
